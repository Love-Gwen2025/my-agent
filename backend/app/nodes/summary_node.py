"""
DeepSearch æ€»ç»“èŠ‚ç‚¹ (Summary Node)

è´Ÿè´£æ ¹æ®æ”¶é›†åˆ°çš„å‚è€ƒèµ„æ–™ï¼Œç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆæ€§å›ç­”ã€‚
"""

from typing import Any

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from loguru import logger

from app.utils.content import extract_text_content

SUMMARY_PROMPT = """# è”ç½‘å‚è€ƒèµ„æ–™
{reference}

# å½“å‰ç¯å¢ƒä¿¡æ¯
{meta_info}

# ä»»åŠ¡
- ç›´æ¥å›ç­”ç”¨æˆ·é—®é¢˜ï¼Œä¸è¦é‡å¤æœç´¢å…³é”®è¯æˆ–æŸ¥è¯¢è¯­å¥ã€‚
- ä¼˜å…ˆå‚è€ƒã€Œè”ç½‘å‚è€ƒèµ„æ–™ã€ä¸­çš„ä¿¡æ¯è¿›è¡Œå›å¤ã€‚
- å›å¤è¯·ä½¿ç”¨æ¸…æ™°ã€ç»“æ„åŒ–ï¼ˆåºå·/åˆ†æ®µç­‰ï¼‰çš„è¯­è¨€ï¼Œç¡®ä¿ç”¨æˆ·è½»æ¾ç†è§£å’Œä½¿ç”¨ã€‚
- å¦‚æœå›å¤å†…å®¹ä¸­å‚è€ƒäº†èµ„æ–™ï¼Œè¯·åŠ¡å¿…åœ¨æ­£æ–‡çš„æ®µè½ä¸­å¼•ç”¨å¯¹åº”çš„å‚è€ƒç¼–å·ï¼Œä¾‹å¦‚[1][2]
- å›ç­”çš„æœ€åéœ€è¦åˆ—å‡ºå·²å‚è€ƒçš„æ‰€æœ‰èµ„æ–™ä¿¡æ¯ã€‚æ ¼å¼å¦‚ä¸‹ï¼š[å‚è€ƒç¼–å·] èµ„æ–™åç§°
ç¤ºä¾‹ï¼š
[1] ç«å±±å¼•æ“
[2] ç«å±±æ–¹èˆŸå¤§æ¨¡å‹æœåŠ¡å¹³å°

# ç”¨æˆ·é—®é¢˜
{question}

# é‡è¦æç¤º
è¯·ç›´æ¥å¼€å§‹å›ç­”é—®é¢˜ï¼Œä¸è¦è¾“å‡ºæœç´¢è¯ã€æŸ¥è¯¢å…³é”®è¯æˆ–"æ— éœ€æ£€ç´¢"ç­‰å†…å®¹ã€‚

# ä½ çš„å›ç­”ï¼š(ç›´æ¥å¼€å§‹æ­£æ–‡)
"""


def format_references_for_summary(references: dict[str, list[str]]) -> str:
    """å°†å‚è€ƒèµ„æ–™æ ¼å¼åŒ–ä¸ºå¸¦ç¼–å·çš„æ–‡æœ¬"""
    if not references:
        return "æš‚æ— å‚è€ƒèµ„æ–™"

    output = ""
    ref_idx = 1
    for query, results in references.items():
        output += f"\nã€æŸ¥è¯¢ '{query}' å¾—åˆ°çš„ç›¸å…³èµ„æ–™ã€‘\n"
        for result in results:
            output += f"[{ref_idx}] {result}\n"
            ref_idx += 1
    return output


def create_summary_node(model):
    """
    åˆ›å»ºæ€»ç»“èŠ‚ç‚¹

    Args:
        model: LangChain æ¨¡å‹å®ä¾‹

    Returns:
        èŠ‚ç‚¹å‡½æ•°
    """
    from datetime import datetime

    async def summary_node(state: dict[str, Any]) -> dict[str, Any]:
        """
        æ€»ç»“èŠ‚ç‚¹ï¼šæ ¹æ®å‚è€ƒèµ„æ–™ç”Ÿæˆæœ€ç»ˆå›ç­”ã€‚
        """
        references = state.get("references", {})
        question = state.get("question", "")
        messages = state.get("messages", [])

        # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„ questionï¼Œä»æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯æå–
        if not question:
            for msg in reversed(messages):
                if isinstance(msg, HumanMessage):
                    question = msg.content
                    break

        logger.info(f"ğŸ“ Generating summary for: {question[:50]}...")
        logger.info(f"ğŸ“š Using {len(references)} reference groups")

        # æ„å»º prompt
        prompt = SUMMARY_PROMPT.format(
            reference=format_references_for_summary(references),
            meta_info=f"å½“å‰æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}",
            question=question,
        )

        try:
            response = await model.ainvoke(
                [
                    SystemMessage(
                        content="ä½ æ˜¯ä¸€ä¸ªæ·±åº¦ç ”ç©¶åŠ©æ‰‹ï¼Œæ“…é•¿ç»¼åˆå¤šæ–¹èµ„æ–™ç»™å‡ºå…¨é¢ã€å‡†ç¡®çš„å›ç­”ã€‚"
                    ),
                    HumanMessage(content=prompt),
                ]
            )

            summary = extract_text_content(response.content).strip()
            logger.info(f"ğŸ“ Summary generated: {len(summary)} chars")

            # è¿”å› AI æ¶ˆæ¯
            return {
                "messages": [AIMessage(content=summary)],
                # æ¸…ç†ä¸´æ—¶çŠ¶æ€
                "search_queries": [],
            }

        except Exception as e:
            logger.error(f"Summary generation failed: {e}")
            return {
                "messages": [AIMessage(content=f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼š{e}")],
                "search_queries": [],
            }

    return summary_node
