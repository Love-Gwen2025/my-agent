"""
èŠå¤©æœåŠ¡ v3 - çŸ¥è¯†åº“é›†æˆæ¶æ„

æ¶æ„ï¼š
1. åªä¼  thread_id + checkpoint_idï¼ŒLangGraph è‡ªåŠ¨ç®¡ç†å†å²
2. çŸ¥è¯†åº“ RAG è‡ªåŠ¨é›†æˆåˆ° context_retrieval èŠ‚ç‚¹
3. DeepSearch æ¨¡å¼æ”¯æŒçŸ¥è¯†åº“é¢„æ£€æŸ¥
4. æ¯è½®ç»“æŸæŒä¹…åŒ–åˆ°æ•°æ®åº“ï¼ˆç”¨äºå±•ç¤ºå’Œå®¡è®¡ï¼‰
"""

import json
from collections.abc import AsyncIterator

from langchain_core.messages import HumanMessage, SystemMessage
from loguru import logger
from sqlalchemy.ext.asyncio import AsyncSession

from app.agent.graph import create_default_agent
from app.core.checkpointer import create_checkpointer
from app.core.constants import AI_SENDER_ID, MAX_TITLE_LENGTH
from app.core.settings import Settings
from app.services.conversation_service import ConversationService
from app.services.embedding_service import EmbeddingService
from app.services.langfuse_service import get_langfuse_service
from app.services.model_service import ModelService
from app.tasks.embedding_tasks import store_message_embedding_task
from app.utils.content import extract_text_content

# ç³»ç»Ÿæç¤ºè¯
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©å›ç­”é—®é¢˜ï¼š
- rag_search: æœç´¢å†å²å¯¹è¯ä¸­çš„ç›¸å…³å†…å®¹
- web_search: åœ¨äº’è”ç½‘ä¸Šæœç´¢å®æ—¶ä¿¡æ¯
- get_current_time: è·å–å½“å‰æ—¶é—´
- simple_calculator: è¿›è¡Œæ•°å­¦è®¡ç®—

ç³»ç»Ÿä¼šè‡ªåŠ¨ä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯å¹¶æä¾›ç»™ä½ å‚è€ƒã€‚
è¯·æ ¹æ®ç”¨æˆ·çš„é—®é¢˜å’Œæä¾›çš„å‚è€ƒèµ„æ–™è¿›è¡Œå›ç­”ã€‚ä¿æŒå›ç­”ç®€æ´ã€å‡†ç¡®ã€æœ‰å¸®åŠ©ã€‚"""


class ChatService:
    """
    èŠå¤©æœåŠ¡ v3 - çŸ¥è¯†åº“é›†æˆæ¶æ„

    ç‰¹æ€§ï¼š
    1. checkpoint_id åˆ†æ”¯ï¼šæ”¯æŒä»å†å²ä»»æ„ç‚¹åˆ†å‰
    2. ä»£è¯æ¶ˆè§£ï¼šRewriteNode è‡ªåŠ¨å¤„ç†
    3. çŸ¥è¯†åº“ RAGï¼šcontext_retrieval èŠ‚ç‚¹è‡ªåŠ¨æ£€ç´¢
    4. å·¥å…·è‡ªä¸»è°ƒç”¨ï¼šæ¨¡å‹å†³å®šæ˜¯å¦è°ƒç”¨ RAG/æœç´¢
    5. æµå¼è¾“å‡ºï¼šé€ token æ¨é€
    """

    def __init__(
        self,
        conversation_service: ConversationService,
        model_service: ModelService | None = None,
        embedding_service: EmbeddingService | None = None,
        settings: Settings | None = None,
    ):
        self.conversation_service = conversation_service
        self.model_service = model_service
        self.embedding_service = embedding_service
        self.settings = settings

    async def _create_title(self, msg: str) -> str:
        """
        æ ¹æ®æ¶ˆæ¯å†…å®¹è‡ªåŠ¨ç”Ÿæˆä¼šè¯æ ‡é¢˜

        ä½¿ç”¨æ„é€ æ—¶æ³¨å…¥çš„ model_serviceï¼Œé¿å…æ¯æ¬¡åˆ›å»ºæ–°å®ä¾‹
        """
        prompt = f"""
æ ¹æ®ä¼ å…¥çš„æ¶ˆæ¯,ç”Ÿæˆä¸€ä¸ª5-10å­—å·¦å³çš„æ ‡é¢˜,å†…å®¹åŠ›æ±‚å‡†ç¡®,ç®€æ˜,æ‰¼è¦ã€‚
åªè¾“å‡ºæ ‡é¢˜æœ¬èº«ï¼Œä¸è¦åŠ å¼•å·æˆ–å…¶ä»–å†…å®¹ã€‚
æ¶ˆæ¯: {msg}
æ ‡é¢˜:"""
        # ä½¿ç”¨å·²æ³¨å…¥çš„ model_serviceï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›æˆªæ–­çš„æ¶ˆæ¯ä½œä¸ºæ ‡é¢˜
        if self.model_service:
            response = await self.model_service.chat(prompt)
            return response.strip()[:MAX_TITLE_LENGTH]
        return msg[:MAX_TITLE_LENGTH]

    # _ensure_string å·²ç§»é™¤ï¼Œä½¿ç”¨ app.utils.content.extract_text_content æ›¿ä»£

    def _has_model(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²é…ç½®æ¨¡å‹æœåŠ¡"""
        return self.model_service is not None

    async def _get_model_for_user(
        self,
        user_id: int,
        model_id: str | None,
        db: AsyncSession | None,
    ):
        """
        æ ¹æ® model_id è·å–å¯¹åº”çš„æ¨¡å‹

        é€»è¾‘ï¼š
        1. å¦‚æœæ²¡æœ‰ä¼  model_idï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤é…ç½®çš„ ModelService
        2. å¦‚æœä¼ äº† model_idï¼ŒæŸ¥è¯¢ç”¨æˆ·æ¨¡å‹å¹¶åˆ›å»ºå¯¹åº”çš„ ModelService

        Returns:
            ChatModel å®ä¾‹ï¼ˆLangChain å…¼å®¹ï¼‰
        """
        from app.services.user_model_service import UserModelService

        # æ²¡æœ‰ä¼  model_idï¼Œä½¿ç”¨ç³»ç»Ÿé»˜è®¤æ¨¡å‹
        if not model_id:
            if self.model_service:
                return self.model_service.get_model()
            return None

        # æœ‰ model_idï¼ŒæŸ¥è¯¢ç”¨æˆ·æ¨¡å‹
        if db:
            user_model_service = UserModelService(db)
            # ç›´æ¥æ ¹æ® ID è·å–å¹¶è§£å¯†
            target_model = await user_model_service.get_with_decrypted_key(user_id, int(model_id))

            if target_model:
                model_service = ModelService.from_user_model(target_model)
                logger.info(f"Using user model: {target_model.model_name} (id={model_id})")
                return model_service.get_model()

        # å›é€€åˆ°ç³»ç»Ÿæ¨¡å‹
        logger.warning(f"User model {model_id} not found, using system default")
        if self.model_service:
            return self.model_service.get_model()
        return None

    def _build_langgraph_config(
        self,
        conversation_id: int,
        db: AsyncSession | None,
        checkpoint_id: str | None = None,
    ) -> dict:
        """
        æ„å»º LangGraph é…ç½®

        Args:
            conversation_id: ä¼šè¯ ID
            db: æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äº RAGï¼‰
            checkpoint_id: å¯é€‰çš„ checkpoint IDï¼ˆç”¨äº regenerate å›é€€ï¼‰

        Returns:
            LangGraph é…ç½®å­—å…¸
        """
        config = {
            "configurable": {
                "thread_id": str(conversation_id),
                "embedding_service": self.embedding_service,
                "db_session": db,
                "conversation_id": conversation_id,
            }
        }
        if checkpoint_id:
            config["configurable"]["checkpoint_id"] = checkpoint_id
        return config

    def _format_sse_event(self, event_type: str, conversation_id: int, **kwargs) -> str:
        """
        æ ¼å¼åŒ– SSE äº‹ä»¶ä¸º JSON

        Args:
            event_type: äº‹ä»¶ç±»å‹ (chunk/tool_start/tool_end/done)
            conversation_id: ä¼šè¯ ID
            **kwargs: é¢å¤–çš„äº‹ä»¶æ•°æ®

        Returns:
            JSON æ ¼å¼çš„å­—ç¬¦ä¸²
        """
        return json.dumps(
            {"type": event_type, "conversationId": str(conversation_id), **kwargs},
            ensure_ascii=False,
        )

    async def _prepare_stream_context(
        self,
        user_id: int,
        conversation_id: int,
        content: str,
        model_code: str | None,
        regenerate: bool,
        parent_message_id: int | None,
    ) -> tuple:
        """
        å‡†å¤‡æµå¼å¯¹è¯çš„ä¸Šä¸‹æ–‡

        Returns:
            (conversation, generated_title, user_message, parent_checkpoint_id)
        """
        # 1. æ ¡éªŒä¼šè¯å½’å±
        conversation = await self.conversation_service.ensure_owner(conversation_id, user_id)

        # 2. é¦–æ¬¡æ¶ˆæ¯æ—¶ç”Ÿæˆæ ‡é¢˜
        generated_title = None
        if not conversation.current_message_id:
            generated_title = await self._create_title(content)
            await self.conversation_service.modify_conversation(
                user_id, conversation_id, generated_title
            )

        # 3. æŒä¹…åŒ–ç”¨æˆ·æ¶ˆæ¯ï¼ˆregenerate æ¨¡å¼è·³è¿‡ï¼‰
        user_message = None
        if not regenerate:
            user_message = await self.conversation_service.persist_message(
                conversation_id=conversation_id,
                sender_id=user_id,
                role="user",
                content=content,
                content_type="TEXT",
                model_code=model_code,
                parent_id=parent_message_id,
            )

        # 4. å¤„ç† regenerate å›é€€
        parent_checkpoint_id = None
        if regenerate and parent_message_id:
            parent_msg = await self.conversation_service.get_message_by_id(parent_message_id)
            if parent_msg and parent_msg.checkpoint_id:
                parent_checkpoint_id = parent_msg.checkpoint_id
                logger.info(f"[stream] Rollback to checkpoint: {parent_checkpoint_id}")

        return conversation, generated_title, user_message, parent_checkpoint_id

    async def stream(
        self,
        user_id: int,
        conversation_id: int,
        content: str,
        model_code: str | None = None,
        model_id: str | None = None,
        regenerate: bool = False,
        parent_message_id: int | None = None,
        db: AsyncSession | None = None,
        mode: str = "chat",
        knowledge_base_ids: list[int] | None = None,
    ) -> AsyncIterator[str]:
        """
        æµå¼å¯¹è¯ - ä½¿ç”¨ LangGraph åŸç”ŸçŠ¶æ€ç®¡ç†

        æµç¨‹ï¼š
        1. æ ¡éªŒä¼šè¯å½’å±
        1.1 å¦‚æœæ˜¯é¦–æ¬¡å‘é€æ¶ˆæ¯ï¼Œè‡ªåŠ¨ç”Ÿæˆæ ‡é¢˜
        2. æŒä¹…åŒ–ç”¨æˆ·æ¶ˆæ¯ï¼ˆregenerate æ¨¡å¼ä¸‹è·³è¿‡ï¼‰
        3. è®¾ç½® RAG ä¸Šä¸‹æ–‡
        4. è°ƒç”¨ LangGraphï¼ˆè‡ªåŠ¨åŠ è½½å†å²ã€æ‰§è¡Œå·¥å…·ã€æ£€ç´¢çŸ¥è¯†åº“ï¼‰
        5. æµå¼è¾“å‡º
        6. æŒä¹…åŒ–åŠ©æ‰‹å›å¤

        Args:
            user_id: ç”¨æˆ· ID
            conversation_id: ä¼šè¯ ID
            content: ç”¨æˆ·æ¶ˆæ¯å†…å®¹
            model_code: æ¨¡å‹ç¼–ç ï¼ˆä¿ç•™å…¼å®¹ï¼‰
            model_id: ç”¨æˆ·æ¨¡å‹ IDï¼Œä¼ æ­¤å‚æ•°åˆ™ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹
            regenerate: é‡æ–°ç”Ÿæˆæ¨¡å¼ï¼Œè·³è¿‡ç”¨æˆ·æ¶ˆæ¯æŒä¹…åŒ–
            parent_message_id: çˆ¶æ¶ˆæ¯ IDï¼Œç”¨äºæ„å»ºæ¶ˆæ¯æ ‘
            db: æ•°æ®åº“ä¼šè¯ï¼ˆç”¨äº RAGï¼‰
            mode: å¯¹è¯æ¨¡å¼ ("chat" | "deep_search")
            knowledge_base_ids: å¯ç”¨çš„çŸ¥è¯†åº“ ID åˆ—è¡¨

        Yields:
            JSON æ ¼å¼çš„ SSE æ•°æ®
        """
        # 1. å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆæ ¡éªŒå½’å±ã€ç”Ÿæˆæ ‡é¢˜ã€æŒä¹…åŒ–ç”¨æˆ·æ¶ˆæ¯ã€å¤„ç† regenerateï¼‰
        _, generated_title, user_message, parent_checkpoint_id = await self._prepare_stream_context(
            user_id, conversation_id, content, model_code, regenerate, parent_message_id
        )

        full_reply = []
        placeholder_message_id = -1

        # 2. æ„å»º LangGraph config
        config = self._build_langgraph_config(conversation_id, db, parent_checkpoint_id)

        logger.info(
            f"[stream] regenerate={regenerate}, parent_checkpoint_id={parent_checkpoint_id}"
        )

        # ç”¨äºåœ¨ checkpointer ä¸Šä¸‹æ–‡å¤–è®¿é—®çš„å˜é‡
        latest_checkpoint_id = None

        # åŠ¨æ€è·å–æ¨¡å‹ï¼ˆæ ¹æ® model_id é€‰æ‹©ç”¨æˆ·æ¨¡å‹æˆ–ç³»ç»Ÿé»˜è®¤æ¨¡å‹ï¼‰
        model = await self._get_model_for_user(user_id, model_id, db)

        if model:
            async with create_checkpointer(self.settings) as checkpointer:
                # åˆ›å»ºå¸¦å·¥å…·çš„ Agent
                graph = create_default_agent(
                    model=model,
                    checkpointer=checkpointer,
                    enable_rewrite=True,
                )

                # æ„å»ºè¾“å…¥æ¶ˆæ¯
                # ç»™ SystemMessage å›ºå®š IDï¼Œé˜²æ­¢ LangGraph é‡å¤è¿½åŠ 
                if regenerate and parent_checkpoint_id:
                    # é‡æ–°ç”Ÿæˆæ—¶ï¼Œä¸æ·»åŠ æ–°æ¶ˆæ¯ï¼Œç›´æ¥ä»çˆ¶ checkpoint ç»§ç»­æ‰§è¡Œ
                    # è¿™æ ·æ–°ç”Ÿæˆçš„ checkpoint ä¼šæˆä¸ºåŸ checkpoint çš„å…„å¼Ÿ
                    input_messages = []
                else:
                    input_messages = [
                        SystemMessage(content=SYSTEM_PROMPT, id="sys_instruction"),
                        HumanMessage(content=content),
                    ]

                # æ„å»º Graph è¾“å…¥çŠ¶æ€
                # åŒ…å«çŸ¥è¯†åº“é…ç½®å’Œä¾èµ–æ³¨å…¥
                graph_input = {
                    "messages": input_messages,
                    "mode": mode,
                    "question": content,
                    "search_queries": [],
                    "references": {},
                    "planning_rounds": 0,
                    # çŸ¥è¯†åº“ç›¸å…³
                    "knowledge_base_ids": knowledge_base_ids or [],
                    "history_context": "",
                    "kb_context": "",
                    # ä¾èµ–æ³¨å…¥ï¼ˆä»¥ _ å¼€å¤´ï¼Œä¾›èŠ‚ç‚¹å†…éƒ¨ä½¿ç”¨ï¼‰
                    "_embedding_service": self.embedding_service,
                    "_db_session": db,
                    "_conversation_id": conversation_id,
                }

                # ä½¿ç”¨ astream_events è·å¾— token çº§æµå¼è¾“å‡º
                logger.info(
                    f"[stream] Starting graph with mode={mode}, "
                    f"kb_ids={knowledge_base_ids or []}"
                )

                # æ³¨å…¥ Langfuse callbackï¼ˆå¦‚æœå¯ç”¨ï¼‰
                langfuse_service = get_langfuse_service(self.settings)
                langfuse_handler = langfuse_service.get_callback_handler(
                    user_id=str(user_id) if user_id else None,
                    session_id=str(conversation_id),
                    trace_name=f"{mode}_chat",
                    metadata={"mode": mode, "regenerate": regenerate},
                )
                if langfuse_handler:
                    config["callbacks"] = [langfuse_handler]
                    logger.info("ğŸ“Š Langfuse tracing enabled")

                async for event in graph.astream_events(graph_input, config=config, version="v2"):
                    kind = event.get("event", "")

                    # LLM ç”Ÿæˆçš„ token
                    if kind == "on_chat_model_stream":
                        # è¿‡æ»¤æ‰éæœ€ç»ˆè¾“å‡ºçš„èŠ‚ç‚¹ï¼ˆplanningã€search ç­‰ä¸­é—´èŠ‚ç‚¹ï¼‰
                        # åªè¾“å‡º chatbotï¼ˆæ™®é€šèŠå¤©ï¼‰å’Œ summaryï¼ˆDeepSearch æ€»ç»“ï¼‰èŠ‚ç‚¹çš„å†…å®¹
                        metadata = event.get("metadata", {})
                        node_name = metadata.get("langgraph_node", "")

                        # å…è®¸è¾“å‡ºçš„èŠ‚ç‚¹ç™½åå•
                        output_nodes = {"chatbot", "summary"}
                        if node_name and node_name not in output_nodes:
                            continue  # è·³è¿‡ä¸­é—´èŠ‚ç‚¹çš„è¾“å‡º

                        chunk = event.get("data", {}).get("chunk")
                        if chunk and hasattr(chunk, "content") and chunk.content:
                            # ä½¿ç”¨ç»Ÿä¸€å·¥å…·å‡½æ•°å¤„ç† Gemini æ ¼å¼
                            token = extract_text_content(chunk.content)
                            if token:  # åªå¤„ç†éç©º token
                                full_reply.append(token)
                                yield self._format_sse_event(
                                    "chunk",
                                    conversation_id,
                                    content=token,
                                    messageId=placeholder_message_id,
                                )

                    elif kind == "on_tool_start":
                        tool_name = event.get("name", "unknown")
                        logger.info(f"Tool started: {tool_name}")
                        yield self._format_sse_event("tool_start", conversation_id, tool=tool_name)

                    elif kind == "on_tool_end":
                        tool_name = event.get("name", "unknown")
                        logger.info(f"Tool ended: {tool_name}")
                        yield self._format_sse_event("tool_end", conversation_id, tool=tool_name)

                # åœ¨åŒä¸€ä¸ª checkpointer ä¸Šä¸‹æ–‡ä¸­è·å–æœ€æ–° checkpoint IDï¼Œé¿å…é‡æ–°åˆ›å»ºè¿æ¥
                config_for_list = {"configurable": {"thread_id": str(conversation_id)}}
                try:
                    async for checkpoint_tuple in checkpointer.alist(config_for_list, limit=1):
                        checkpoint = checkpoint_tuple.checkpoint or {}
                        latest_checkpoint_id = checkpoint.get("id")
                        break
                except Exception as exc:
                    logger.error(f"Failed to fetch latest checkpoint: {exc}")

        else:
            # æœªæ¥å…¥æ¨¡å‹æ—¶çš„å›é€€
            fallback = f"æš‚æœªæ¥å…¥æ¨¡å‹ï¼Œå›æ˜¾: {content}"
            full_reply.append(fallback)
            yield self._format_sse_event(
                "chunk", conversation_id, content=fallback, messageId=placeholder_message_id
            )

        # 5. æŒä¹…åŒ–åŠ©æ‰‹æ¶ˆæ¯
        reply_text = "".join(full_reply) if full_reply else ""

        # latest_checkpoint_id å·²åœ¨ä¸Šé¢çš„ checkpointer ä¸Šä¸‹æ–‡ä¸­è·å–

        # AI æ¶ˆæ¯çš„ parent_id æ˜¯ç”¨æˆ·æ¶ˆæ¯çš„ ID
        ai_parent_id = user_message.id if user_message else parent_message_id

        assistant_message = await self.conversation_service.persist_message(
            conversation_id=conversation_id,
            sender_id=AI_SENDER_ID,
            role="assistant",
            content=reply_text,
            content_type="TEXT",
            model_code=model_code,
            token_count=len(reply_text),
            parent_id=ai_parent_id,
            checkpoint_id=latest_checkpoint_id,
        )

        # 6. ä½¿ç”¨ Celery å¼‚æ­¥å­˜å‚¨ embeddingï¼ˆå®Œå…¨è§£è€¦ï¼Œä¸é˜»å¡å“åº”ï¼‰
        if self.embedding_service and self.settings:
            db_url = str(self.settings.database_url)
            if user_message:
                # æ­£å¸¸æ¨¡å¼ï¼šå­˜å‚¨ç”¨æˆ·æ¶ˆæ¯
                store_message_embedding_task.delay(
                    db_url, user_message.id, conversation_id, user_id, "user", content
                )
            # å­˜å‚¨ AI å›å¤
            store_message_embedding_task.delay(
                db_url, assistant_message.id, conversation_id, user_id, "assistant", reply_text
            )
            logger.debug(f"Queued embedding tasks for conversation {conversation_id}")

        # 7. å‘é€å®Œæˆä¿¡å·
        # è·å–ç”¨æˆ·æ¶ˆæ¯ IDï¼ˆregenerate æ—¶ä½¿ç”¨ parent_message_idï¼‰
        user_message_id = user_message.id if user_message else parent_message_id

        yield json.dumps(
            {
                "type": "done",
                "messageId": str(assistant_message.id),
                "conversationId": str(conversation_id),
                "tokenCount": len(reply_text),
                "parentId": str(ai_parent_id) if ai_parent_id else None,
                "userMessageId": str(user_message_id) if user_message_id else None,
                "title": generated_title,  # æ–°ç”Ÿæˆçš„æ ‡é¢˜ï¼ˆå¦‚æœæœ‰ï¼‰
            },
            ensure_ascii=False,
        )

    async def _get_latest_checkpoint_id(
        self,
        conversation_id: int,
    ) -> str | None:
        """
        è·å–æœ€æ–° checkpointIdï¼Œç”¨äºåœ¨ SSE done äº‹ä»¶ä¸­è¿”å›ã€‚
        """
        try:
            # å¤ç”¨ä»…åŒ…å« thread_id çš„é…ç½®ï¼Œç¡®ä¿è¯»å–æœ€æ–°çŠ¶æ€
            config = {"configurable": {"thread_id": str(conversation_id)}}
            async with create_checkpointer(self.settings) as checkpointer:
                # ä½¿ç”¨ alist è·å–æœ€æ–° checkpointï¼Œä»¥ä¾¿è·å– parent_config
                async for checkpoint_tuple in checkpointer.alist(config, limit=1):
                    # CheckpointTuple æ˜¯å¯¹è±¡ï¼Œä½¿ç”¨å±æ€§è®¿é—®
                    checkpoint = checkpoint_tuple.checkpoint or {}
                    parent_config = checkpoint_tuple.parent_config or {}

                    checkpoint_id = checkpoint.get("id")
                    parent_id = None
                    if parent_config:
                        configurable = parent_config.get("configurable", {}) or {}
                        parent_id = configurable.get("checkpoint_id")

                    return checkpoint_id, parent_id
                return None, None
        except Exception as exc:
            logger.error(f"Failed to fetch latest checkpoint info: {exc}")
            return None, None
